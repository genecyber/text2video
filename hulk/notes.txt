config namespace(framework='pytorch', task='text-to-video-synthesis', model={'type': 'latent-text-to-video-synthesis', 'model_args': {'ckpt_clip': 'open_clip_pytorch_model.bin', 'ckpt_unet': 'text2video_pytorch_model.pth', 'ckpt_autoencoder': 'VQGAN_autoencoder.pth', 'max_frames': 16, 'tiny_gpu': 1}, 'model_cfg': {'unet_in_dim': 4, 'unet_dim': 320, 'unet_y_dim': 768, 'unet_context_dim': 1024, 'unet_out_dim': 4, 'unet_dim_mult': [1, 2, 4, 4], 'unet_num_heads': 8, 'unet_head_dim': 64, 'unet_res_blocks': 2, 'unet_attn_scales': [1, 0.5, 0.25], 'unet_dropout': 0.1, 'temporal_attention': 'True', 'num_timesteps': 1000, 'mean_type': 'eps', 'var_type': 'fixed_small', 'loss_type': 'mse'}}, pipeline={'type': 'latent-text-to-video-synthesis'})
device cuda
got a request to *vid2vid* an existing video.
Trying to extract frames from video with input FPS of 15.0. Please wait patiently.
Successfully extracted 37.0 frames from video.
Loading frames: 100%|█████████████████████████████████████████████████████████████████| 37/37 [00:00<00:00, 314.87it/s]
Converted the frames to tensor (1, 37, 3, 256, 256)
Computing latents
STARTING VAE ON GPU
VAE HALVED
Working in vid2vid mode
  0%|                                                                                            | 0/1 [00:00<?, ?it/s]Making a video with the following parameters:
{'prompt': 'Hulk, Michael bay, yell', 'n_prompt': 'text, watermark, copyright, blurry, nsfw, notext, no text', 'steps': 44, 'frames': 37, 'seed': 912119538, 'scale': 9, 'width': 256, 'height': 256, 'eta': 0.0, 'cpu_vae': 'GPU (half precision)', 'device': device(type='cuda'), 'skip_steps': 4, 'strength': 0}
latents torch.Size([1, 4, 37, 32, 32]) tensor(0.1941, device='cuda:0', dtype=torch.float16) tensor(0.9473, device='cuda:0', dtype=torch.float16)
huh tensor(925) tensor([925], device='cuda:0')